,title,Background,Methods,Results,Conclusion
0,Genetic subdivisions within Trypanosoma cruzi(Discrete Typing Units) and their relevance for molecular epidemiology and experimental evolution,"It is probable that Trypanosoma cruzi, the agent of Chagas disease, is the pathogenic microorganism for which intraspecific genetic diversity is the best known. Longstanding interest in this diversity has led many teams currently working on this parasite to follow and even to generate the recent technological progress in biochemical typing, molecular epidemiology* (Terms quoted * are explained in the glossary) and population genetics*. In the early seventies, obtaining knowledge of the population structure of pathogenic microorganisms was a major challenge, since their formal genetics was entirely speculative. The isoenzyme* era gave us the first insights to the genetic diversity of T. cruzi and other parasites. Pioneering studies by Miles et al. [1, 2] showed clearly the existence of three main isoenzyme types, which were called ""principal zymodemes*"". Such zymodemes were taken as units of analysis for epidemiological surveys [3] and hypotheses on T. cruzi pathogenicity [4]. Numerical taxonomy showed their overall clustering [5]. However the biological nature and evolutionary origin of the zymodemes* remained entirely unknown. The specific contribution of our group has been to apply the concepts of population genetics to the study of T. cruzi biochemical and genetic polymorphism. The present paper describes the main results reached in this field by our group and other teams.","The key-point in understanding the population structure of a pathogenic microorganism is its mating system. The classical view, that microbes reproduce clonally, has been upset by the population genetic era. Genetic exchange is very frequent in many microbial species. Whatever its precise cytological mechanism, horizontal gene transfer affects pathogen population structure: it clouds phylogenetic individualization of lineages and renders individual genotypes ephemeral [6]. There is a practical consequence of this for molecular epidemiology: if pathogen multilocus genotypes have a short lifetime, they cannot be conveniently used as epidemiological tracers. The ""clonality/sexuality debate"" has been, therefore, the target of many research groups in the last twenty years, and is still controversial. The approach proposed by us for T. cruzi [7] and by others for bacteria [8] has been to look for the expected consequences of random allelic segregation and unilocus genotype recombination in the natural populations surveyed. If these consequences are not observed, this is taken as circumstantial evidence that gene flow is inhibited. Allelic segregation is surveyed by the classical Hardy-Weinberg equilibrium* statistics. It is a demanding approach when microbial pathogens are considered. First it requires that the ploidy of the organism is known. This is sometimes difficult. As an example, T. cruzi, which was supposed to be diploid [7], is now considered an aneuploid organism according to experimental recombination data [9]. Second, Hardy-Weinberg tests are not applicable to haploid organisms, which is the case for bacteria and human forms of Plasmodium parasites. For these reasons, recombination tests are considered more reliable [10]. They are based on the null hypothesis of free genetic exchange (panmixia) and rely on the analysis of linkage disequilibrium*. The same basic principles are still used in recent contributions to this field of research [6]. Physical obstacles to gene flow (isolation by either time or space or both) can generate linkage disequilibrium* too (Wahlund effect). Means to avoid such biases have been detailed previously [11]. The statistical tests elaborated by our group to detect linkage disequilibrium are communicated in table 1 and will be made available on the internet in a near future. Other tests relying on the same basic principles are available [12]. Linkage disequilibrium* tests are extremely powerful, since the probability of occurrence of a given mutlilocus combination under the panmictic* assumptions is very low if the number of loci is sufficient. Observing repeated multilocus combinations is therefore in itself a strong indication for linkage disequilibrium*, which statistical level of significance is evaluated by the tests (table 1). Apart from predominant clonal* evolution, linkage disequilibrium* can be generated by either cryptic speciation or epidemic clonality (propagation of ephemeral clones in a basically sexual species; [12]). Means to distinguish between these two cases from ""true"" clonal* evolution have been communicated [13].Table 1 Criteria and tests of clonality (after 10)Full size tableThe ""clones*"" observed by a given set of genetic markers will prove to be genetically heterogeneous if a more discriminative marker is used. We have forged the term ""clonet"" to designate a set of stocks that appear genetically identical with a given set of markers in a clonal* species [11].In complement with population genetics, classical phylogenetic analysis is useful to look for possible discrete genetic subdivisions in the species under study. In the case of microbial pathogens, many times, such subdivisions are observed, however they do not fulfill the rigorous criteria of phylogenetic analysis, since some level of horizontal gene transfer renders these subdivisions incompletely isolated from each other. The descriptive concept of ""Discrete Typing Unit"" (DTU) designates a set of stocks that are genetically more similar to each other than to any other stock, and are identifiable by common genetic, molecular, or immunological markers named ""tags"" [14].","T. cruzi still is a paradigmatic case of predominantly clonal evolution. Evidence for lack of Mendelian segregation, an argument taken long ago on the basis of the diploidy hypothesis [7], has been challenged by recent mating experiments showing that T. cruzi is aneuploid [9]. However such results do not falsify the line of evidence based on the analysis of linkage disequilibrium*, which remains valid. As a matter of fact, an impressive congruence of results corroborates the existence of strong linkage disequilibrium* in the agent of Chagas disease [15], even in sylvatic cycles and when each genetic subdivision is analyzed separately. A striking illustration of this linkage disequilibrium* is the existence of a highly significant correlation between independent genetic markers, including isoenzymes [16], Random Primed Amplified Polymorphic DNA* or RAPD [17] and microsatellites [18]. It seems that long-term clonal evolution in T. cruzi has been predominant enough to lead to the individualization of several discrete genetic subdivisions or DTUs [14]. The number of observable DTUs within T. cruzi is a matter of debate. Most studies show the existence of two main subdivisions [13, 19], referred to as T. cruzi I and II [20]. Multilocus markers reliably show a total of six DTUs, one corresponding to T. cruzi I, the others corresponding to subdivisions within T. cruzi II [16, 17, 21]. Classifications based on gene sequencing either support the division into 6 DTUs [22] or indicate a lesser number [23–25]. This illustrates the usefulness of the DTU concept. Two T. cruzi DTUs (2d and 2e; see figure 1) correspond actually to hybrid lineages stabilized by subsequent clonal propagation [9, 25–27]. These lineages do not fulfill the strict criteria of cladistic analysis and actually, they are not clades, since they have two ancestors instead of one, which explains the inconsistency of gene sequence phylogenies. However, they correspond to reliable genetic subdivisions and are identifiable by an impressive set of tags.Figure 1Neighbour joining dendrograms based on the analysis of 20 RAPD* primers (left) and 22 isoenzymatic* loci (right) showing the genetic relationships between 49 Trypanosoma cruzi stocks and T. cruzi marinkellei stock M1117. The scale indicates the Jaccard distance [39] along the branches. Six genetic clusters, or Discrete Typing Units (DTUs [14]), were distinguished and their names are given in the central column between the dendrograms. DTU 1 corresponds to the 1rst major lineage of T. cruzi, while the second major lineage is subdivided into DTUs 2a to e. Diagnostic RAPD fragments and isoenzymatic patterns, which were specifically observed in the stocks of a given cluster of the dendrograms (tags [14]), are indicated at the corresponding nodes (after [17]).Full size image","Currently, the 6 DTUs are the most reliable subdivisions of T. cruzi. They appear as robust units of analysis for molecular epidemiology studies. DTU 1 (= T. cruzi I; [20]) corresponds to all genotypes related to the formerly described ""principal zymodeme 1"" [1, 2]. It is a very broad and heterogeneous group. Its epidemiological and geographical specificity is low. It is found on the entire range of Chagas disease, from southern USA to Argentina. It can be found in domestic cycles in Andean countries as well as in Amazonian sylvatic cycles. It is very frequent in chronic cases of Chagas disease. Identifying therefore an isolate as DTU 1 has a very low predictive value on its expected properties. The case is different for the 5 DTUs that subdivide T. cruzi II [20]. Their epidemiological and geographical specificity is clearer, and identifying them is therefore informative. DTU2a and DTU2b correspond to stocks of ""principal zymodemes III and II"", respectively [1, 2]. Interestingly, DTU 2b (zymodeme III), which was until now only known from Sylvatic cycles, has been recently recorded in chronic cases of Chagas disease in Ecuador [28]. This shows that our knowledge of the epidemiological implications of T. cruzi genetic variability still is incomplete. The epidemiological relevance of the 6 DTUs has been analyzed in details by Barnabé et al. [16]. It is relevant to identify also the clonets within each DTU for finer epidemiological studies. It is desirable for this purpose to perform a discriminative genetic characterization. Our group routinely uses 22 isoenzyme loci and 20 RAPD primers [16, 17]."
1,Genetic susceptibility of serum cholesterol and triglyceride in Chinese Han children and adolescents,"Genetic studies might provide new insights into the biological mechanisms underlying lipid metabolism and risk of Cardiovascular disease. We therefore conducted a study to identify genetic determinants of triglycerides(TG), high-density lipoprotein cholesterol (HDL-C) and non-high-density lipoprotein cholesterol (non-HDL-C).",We investigated adiponectin receptor 1(AdipoR1) gene rs10920533 associated with non-HDL-C(case: 109 subjects with high non-HDL-C; control: 701 subjects with normal non-HDL-C); apolipoprotein A5(ApoA5) gene -1131T>C and -3A>G associated with TG(case: 245 subjects with high TG; control: 595 subjects with normal TG); glucocorticoid receptor(GR) gene rs12521436 associated with HDL-C(case: 129 subjects with low HDL-C; control: 722 subjects with normal HDL-C). Matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) was used for SNP genotyping.,"Compared to AdipoR1 rs10920533 GG genotype, carriers of AG had lower non-HDL-C, but the differences did not reach statistical significance; after adjusted for BMI, TC, TG and LDL-C with multi-factor logistic regression, AdipoR1 rs10920533 AG genotype was not independently associated with the non-HDL-C level. APOA5 -1131CC and -3GG had higher TG compared to common allele homozygotes, respectively, but the differences did not reach statistical significance; after adjusted for age, WC, TC, HDL-C and LDL-C with multi-factor logistic regression, the risk of -1131CC genotype was found significantly increased than that of the TT genotype, the OR value of which was 2.667(95% confidence interval: 1.413-5.033) and the risk of -3GG genotype was found significantly increased than that of the AA genotype, the OR value of which was 2.561(95% confidence interval: 1.342-4.888). For GR rs12521436, compared to the homozygous for the G allele, carriers of A allele had lower levels of HDL-C, but the differences did not reach statistical significance; after adjusted for BMI, TC and TG with multi-factor logistic regression the result show that a more increased risk for lower HDL-C in subjects with the A carriers, which the crude OR for subjects with A allele was 1.937 (95% confidence interval: 1.166-3.215) relative to GG carriers.",Our results demonstrate an independent risk for ApoA5 -1131T>C and -3A>G gene polymorphisms in the development of a elevated TG level. GR rs12521436 gene polymorphisms might contribute to a reduced level of HDL-C. The single nucleotide polymorphism rs10920533 in the AdipoR1 gene is not associated with non-HDL-C level.
2,The clinical and molecular genetic study of 20 Silver Russell Syndrome cases,"To analyze the genetic pathogenesis and improve the accuracy of the diagnosis of the disease, this study reported the clinical features of 20 patients with Silver Russell Syndrome (SRS) in the Beijing Children's Hospital and detected the chromosome 11p15 imprinting defects in 16 patients of them.","20 SRS cases diagnosed in Beijing Children’s Hospital from 2006 to 2011 were studied retrospectively for clinical manifestations, physical signs, laboratory examinations and respond of GH treatment. We compared with 3 different diagnostic criteria and used the methylation-specific multiplex ligation dependent probe amplification (MS-MLPA) method to detect the chromosome 11p15 imprinting defects in 16 patients of them, meanwhile take 10 normal control.","We collected 20 SRS patients over a period from 2006 to 2011 with 3 criteria. The concordance is 90%. Include fifteen males and five females, age range 0.08~12.17yr. The most chief complaint is short, 85%. Then, it is asymmetry (5%), and external genital abnormalities (10%). The clinical characteristics with the frequencies accounted for over 80% included small for gestation age(SGA), postnatal growth retardation, craniofacial dysmorphism, asymmetry and super thin of whole body, extremely limbs, fifth finger clinodactyly, BMI<-2SDS and height was obviously lagging behind the bone age. Six of them had used the growth hormone for 3-months to 12 Months. The growth velocity is from 4cm to 10cm/year. In the 16 patients, 6 patients were found hypomethylation in chromosome 11p15 ICR1, the other one with chromosome 11p15 ICR1 hypomethylation and ICR2 hypermethylation may be the result of the maternal chromosome 11p15 uniparental disomy. Another case had duplication of the maternal chromosome 11p15 fragment. Two cases had good effects of GH treatment, one with chromosome 11p15 ICR1 hypomethylation and the other one was normal.","The top 3 clinical features in SRS are ① growth retardation include SGA and/or postnatal. ② Malformation include craniofacial dysmorphism, asymmetry of face and/or limbs, fifth finger clinodactyly. ③ Super severe low BMI and height was obviously lagging behind bone age. No laboratory and imagination specificity. Chromosome 11p15 imprinting defect is the major genetic disturbance in SRS, about 50%, and ICR1 hypomethylation is the predominant molecular alteration. The MS-MLPA is a technique for detecting all chromosome 11p15 imprinting defects of SRS. It is useful for the study of the genetic aspects of SRS. The relation between the respond of GH treatment and genetic changes is uncertainty."
3,Comparison of artificial neural network analysis with other multimarker methods for detecting genetic association,"As discussed recently [1], when genetic markers are used to attempt to detect association with a disease phenotype there are grounds for expecting that, in some circumstances at least, power will be gained by analysing groups of markers jointly rather than considering each marker individually. However uncertainty remains as to the best method for carrying out such a multimarker analysis. Probably the most commonly used approach at present is to carry out haplotype-based analyses, in which haplotypes are estimated from phase-unknown genotypes and then the estimated haplotype frequencies in case and control groups are compared, for example using the GENECOUNTING program [2, 3]. From a theoretical point of view this approach may not be optimal for a number of reasons. Typically one will use a likelihood ratio test for heterogeneity of haplotype frequencies and this will have a number of degrees of freedom equal to one less than the number of haplotypes estimated to be present, typically 2m if there are m biallelic markers [4]. Using this many degrees of freedom may result in a conservative test. This problem can partly be addressed by permutation-testing to assess statistical significance but it is still clear that the approach is not optimal. For example, suppose there is a 3-marker haplotype associated with disease but one carries out the test including an additional 2 markers which are not associated. Within the 32 possible 5-marker haplotypes there will be 4 which contain the associated 3-marker haplotype but these 4 haplotypes will not be treated as in any way ""similar"" to each other and the signal from them may well be drowned out by the noise from the other markers. Rather than test for heterogeneity of haplotype frequencies between cases and controls one may seek to model the effects of haplotypes on risk of affection. This different, albeit related, approach is implemented in the UNPHASED program and involves estimating haplotype frequencies and then carrying out logistic regression analysis with the individual haplotypes modelled to confer different risks of affection [5, 6].An alternative approach to utilising multimarker data is to model the effect of each marker separately, generally producing tests with fewer degrees of freedom. A previous investigation [7] compared such locus scoring tests to haplotype scoring tests and found that former were more powerful. In our own investigations [1], we compared a locus-based test implementing logistic regression with a haplotype-based heterogeneity test and found that they had similar power to detect a single pathogenic mutation. The UNPHASED program incorporates an option to treat alleles, rather than haplotypes, as risk factors, resulting in a similar method of analysis [6]. There are theoretical reasons to expect that haplotype-based methods might be relatively more powerful if more than one mutation were present. This is because different haplotypes might be in linkage disequilibrium (LD) with different mutations and distortions in their freqeuencies might be easier to detect than effects on the allele frequencies of individual markers. In this situation one might expect that locus scoring tests would lose power through their failure to consider haplotypic effects.I and colleagues have proposed an alternative method for analysing multimarker data using artificial neural networks (ANNs) [8, 9]. ANNs are designed to detect patterns in input data which may match to output data even if the nature of such patterns is not known a priori. By training an ANN to match multimarker genotypes to disease phenotype the hope is that it may be able to detect association which may be based on one marker or several, full haplotypes or partial ones and one or more different haplotypes. We showed that this method can be more powerful than tests based on single markers but it has not previously been compared it with other multimarker approaches.Here, I investigate the relative power of haplotype analysis, logistic regression, UNPHASED analyses and ANN analysis using real genotype data to provide information on SNP allele frequencies and LD relationships such as would be found in case-control association studies.","The main results obtained from this investigation are displayed in Table 1. A number of observations are worthy of note.Table 1 Relative power of heterogeneity tests, logistic regression, UNPHASED analyses and ANN analysis to detect association at p < 0.01 in a case control study using different disease modelsFull size tableFirstly, in terms of absolute power we can see that for samples consisting of a few hundred cases and controls association may well not be detected even using closely spaced SNPs. Of course, the ability to detect association is crucially dependent on sample size and disease model as well as LD relationships between polymorphisms. The disease models used here incorporate relative risks of 2 or 3 and with these sample sizes power to detect association at p < 0.01 ranges from 29% to 68%.For the haplotype-based test, with the exception of a couple of datasets in which power was equal, permutation testing was always more powerful than referring to the asymptotic chi-squared distribution, in most cases to only a small degree although in one case with a power difference as high as 6%. By this we mean that the permutation test more often produced a p value of 0.01 or less. The minimum empirical p value that can be estimated with 999 permutations is 0.001 and the asymptotic test often produced a much lower value than this, meaning that the average p value for the asymptotic test was more highly significant. Nevertheless, in terms of ability to reach the threshold set a priori permutation testing was more powerful. This indicates that using the asymptotic distribution does produce a test which is somewhat conservative.The power was very similar for the permutation-based test of heterogeneity of haplotype frequencies, the logistic regression test for effects of individual loci and both the haplotype-based and allele-based analyses implemented in UNPHASED. In fact, the power of the haplotype-based analysis used by UNPHASED was consistently higher than that of the allele-based analysis but only to a small extent, amounting on average to a difference of only 1%.The ANN analysis tended to have a higher power than other methods, albeit not consistently so in that for a couple of models the haplotype-based analysis implemented in UNPHASED had higher power. Across all models, the difference in power between ANN analysis and the heterogeneity test of haplotype frequencies based on asymptotic p values was statistically significant (chi-squared = 10.4, 1 df, p = 0.0012). However, other comparisons between tests were not statistically significant. The power advantage of the ANN analysis over the heterogeneity test of haplotype frequencies was higher for the models in which the relative risk was set to 3 rather than 2 (p = 0.03) and for those using the more closely spaced markers (p = 0.01) but there was no overall difference in power between dominant and recessive models.There was a fairly strong correlation between the p values obtained from the ANN analysis and those obtained from the heterogeneity test of haplotype frequencies (R = 0.71). There was also a high correlation between the t statistic obtained from ANN analysis and the empirical p value (R = 0.91), suggesting that the t statistic could be utilised as a preliminary indicator for genetic association without the necessity to carry out permutation testing. In support of this notion, this t statistic also demonstrated high correlation (R = 0.86) with the p values obtained from the heterogeneity test of haplotype frequencies.","We must reiterate that the results we have obtained are contingent on utilising particular disease models and sample sizes, though the SNP data we have used do reflect real data in terms of marker informativeness, spacing and LD relationships. That said, there do seem to be some interesting implications. Haplotype-based analysis is currently the most widely used method and can probably be fairly regarded as standard. However under the conditions of this investigation it is shown to be by no means the most powerful method and is out-performed by logistic regression, UNPHASED and ANN analysis. When the asymptotic p value is used for the haplotype analysis, as in practice would usually be the case for an initial screen, ANN analysis has a power advantage which would have practical implications in the real world. When one considers the vast resources which can go into performing a case-control association study it would be disastrous for an association to be missed through utilising a test which was several percentage points less powerful than another.Quantifying relative power by assessing the proportion of times each method of analysis yields a particular target p value might imply that one was taking a very simplistic view of how genetic investigations were carried out. This would be that a set of markers were genotyped and then analysed using only one method of analysis in groups containing a number of markers which had been specified in advance. Regions containing a group which reached the target p value would then be subjected to intensive study in an effort to identify variants directly influencing risk of affection while other regions would be ignored. In practice the situation would likely be far more complicated. A variety of methods of analysis might be used, including single marker and multi-marker analyses containing different numbers and combinations of markers, probably selected on an ad hoc basis. If a p value just failed to reach some arbitrary level of significance then it would not simply be ignored but the region might be kept under consideration for the future, although afforded a lower priority. Regions yielding the most highly significant results would probably be examined first but the failure to reach a target p value would not necessarily mean the difference between detecting an association and missing it entirely.With this caveat in mind, it does nevertheless seem that this investigation demonstrates that haplotype based analysis, as commonly used, is at least in some situations not the best way to detect association using multi-marker data. This would confirm what might expect from theoretical considerations, in particular that there is a failure to treat haplotypes which are similar to each other any differently from those which have no alleles at all in common. The pattern-matching abilities of ANN analysis do seem better able to detect the kinds of deviation from random distribution of multi-marker unphased genotypes which are generated when a susceptibility locus is present and in LD with at least some of the marker loci. There is a suggestion from these results that the advantage of ANN analysis may be more pronounced when there is a larger genetic effect and when markers are closer together. However further work would need to be carried out to formally investigate whether there were particular situations in which one the different methods would have different relative merits. As we have mentioned previously [1], a particular situation worthy of study would be that in which different mutations in the same gene can influence risk. From a theoretical point of view one might well expect this to have an important impact but modelling this situation would require more sophisticated simulation software.Although ANN analysis demonstrates superiority in this investigation, it would be unrealistic to recommend that it be widely adopted. One disadvantage is the lack of theory-driven testing for association. The ANN detects some kind of patterns which can be used to distinguish the genotypes of cases from those of controls but the nature of this association is not specified in advance and even after the ANN has been trained to detect a difference the criteria it uses are unclear. This is not a desirable situation. The ANN can output a lists of the genotypes which produce the highest and lowest outputs and perusing these lists may offer some indication of which alleles and combinations of alleles appear to be commoner in cases but this is hardly a rigorous process. There is also a very important practical disadvantage, which is that ANN analysis is slow. For each set of markers one has to go through a cycle of repeated trainings followed by testing and then these cycles need to be repeated many times on permuted data to obtain an empirical p value. As currently implemented, the ANN takes in the region of 20–30 minutes to analyse one set of markers in a few hundred subjects using an ordinary desktop PC and 999 permutations. If one wished to estimate a lower p value, in the region of 0.001, one would need 10 times as many permutations and the analysis would take 10 times as long. One approach which can produce some useful speed benefits is to use sequential sampling to obtain empirical p values [10]. When carrying out permutation testing, rather than setting the number of permuted replicates, n, to a fixed number one instead sets a target for r, the number of times that a permuted replicate should exceed the test statistic obtained from the real dataset. Typically a target for r might be set to a value of 10 or 20. One would also set some maximum value of n to ensure that the procedure did eventually finish. If the target value for r is reached then the empirical significance is given by p = r/n while if the target is not reached before n reaches its maximum value the empirical significance is given by p = (r+1)/(n+1), as used in conventional Monte Carlo testing. This produces a very valuable increase in speed of permutation testing when the p value to be estimated turns out to be non-significant. If there is no association present then one will expect to only perform 2r permutations before the target is reached. With a target of r = 10 then one achieves a 50-fold speed increase compared with using the conventional method with n = 999. This approach would be very useful when analysing large numbers of markers, most of which are expected not to demonstrate association. However sequential sampling does not provide any advantage when the p value to be estimated is in fact small and one will still need to carry out a large number of permutations in order to produce an acceptably accurate estimate. In some genetic investigations multi-marker analysis produces p values which are very small indeed and it would be difficult to obtain these using a Monte Carlo approach. If one carries out a screen using hundreds of thousands of markers then one will not wish to set a threshold of p ≤ 0.01 to designate regions for further consideration, since this would leave one with thousands of candidate regions. However such a threshold might arguably be appropriate if a small number of markers were investigated in a region which was already of interest a priori and it could be noted that in the current investigation p values of this magnitude were sometimes produced from a sample of several hundred subjects and with a disease locus having a moderate effect on risk.An alternative approach to implementing ANN analysis for large numbers of markers might be to consider the raw t statistic as an indicator of association rather than going on to carry out permutation testing. We have previously emphasised that, because the t statistic is obtained by testing the same case and control samples as were used to train the ANN, no formal interpretation can be made for the magnitude of evidence in favour of association from the t statistic on its own. It is simply a measure of how well the network has been able to adapt to the data it has been presented with in terms of finding an algorithm which will match inputs with outputs. A number of confounding factors might theoretically be expected to influence this ability, in particular the allele frequencies of the markers used and the LD relationships between them. However, here we have shown that such concerns may in fact be exaggerated and that the t statistic is in itself a reasonable indicator of association. It is highly correlated both with the empirical p values obtained from permutation testing of ANN analysis (R = 0.91) and with the p obtained from conventional tests for heterogeneity of haplotype frequencies (R = 0.86). In practice this means that if one were to study thousands of markers one might begin by carrying out ANN analysis without permutation testing, knowing that those sets of markers producing the highest values for the t statistic were likely to be those showing the strongest evidence for association. One could then select these sets and carry out permutation testing on them in order to obtain a formal measure of statistical significance.","This investigation demonstrates that in at least some situations standard haplotype-based analysis is less powerful than other methods. Although ANN analysis has performed better we do not envisage that it will be widely taken up as an alternative. However, the results do suggest that there is room to develop new methods which might share the advantages of ANN analysis in terms of implementing a parsimonious approach to detect the patterns of multi-marker genotypes which can be observed when an associated susceptibility locus is present. Such methods might offer useful increases of power. Given the resources which need to be invested in collecting samples of cases and controls and obtaining genotypes it seems sensible to argue that considerable effort should be expended on ensuring that methods of analysis applied to the data obtained are as effective as possible.For now, we suggest that it should be recognised that heterogeneity tests of haplotype frequencies may be intrinsically somewhat conservative. This would imply that when analysing many markers one might set a somewhat lower threshold to indicate which markers were worthy of further investigation, which would include formal permutation testing in order to obtain a reliable p value. With the exception of ANN analysis, which may have a slight advantage, the other methods studied demonstrate similar perfomance to each other under the condtions of these simulations. Investigators could feel reassured about using any of them until further information regarding their relative performance becomes available."
4,"Screening of cacna1a and ATP1A2 genes in hemiplegic migraine: clinical, genetic and functional studies","Hemiplegic migraine (HM) is a rare and severe subtype of autosomal dominant migraine, characterized by a complex aura including some degree of motor weakness. Mutations in three genes (CACNA1A, ATP1A2 and SCN1A) have been detected in familial and in sporadic cases. This genetically and clinically heterogeneous disorder is often accompanied by permanent ataxia, epileptic seizures, mental retardation, and chronic progressive cerebellar atrophy.",To perform an exhaustive mutational screening of the CACNA1A and ATP1A2 genes in 18 HM patients.,"Direct sequencing of PCR amplicons, Multiplex Ligation-dependent Probe Amplification (MLPA), Quantitative Multiplex PCR of Short Fluorescent fragments (QMPSF), heterologous expression and electrophysiology, ouabain survival assay.","We identified four previously described missense CACNA1A mutations (p.Ser218Leu, p.Thr501Met, p.Arg583Gln and p.Thr666Met) and two missense changes in the ATP1A2 gene, the previously described p.Ala606Thr and the novel variant p.Glu825Lys. Additionally, a quantitative analysis was performed to detect exonic duplications or deletions in the CACNA1A gene using MLPA and QMPSF, with negative results. Functional studies were performed for the CACNA1A p.Thr501Met mutation and the ATP1A2 p.Glu825Lys change, the first having been previously described only in association with the EA2 phenotype."
5,Genetic determinants of phenotypic diversity in humans,"Phenotypic variation in humans is a direct consequence of genetic variation, which acts in conjunction with environmental and behavioral factors to produce phenotypic diversity. Genetic variants are classified by two basic criteria: their genetic composition and their frequency in the population. In terms of composition, polymorphisms can be classified as sequence variants or structural variants. Sequence variants range from single nucleotide differences between individuals to 1 kilobase (kb)-sized insertions or deletions (indels) of a segment of DNA (Figure 1) [2]. Larger insertions and deletions, as well as duplications, inversions and translocations, are collectively called structural variants. These variants can range in size from 1 kb to those spanning more than 5 megabases (Mb) of DNA [3].Figure 1Classification of genetic variants by composition. Schematic of sequence and structural variants compared to reference sequence. Sequence variation (indicated by red line) refers to single-nucleotide variants and small (less than 1 kb) indels. Structural variation includes inversions, translocations and copy-number variants, which result in the presence of a segment of DNA in variable numbers compared to the reference sequence, as in duplications, deletions or insertions. Adapted from [4].Full size imageGenetic variants are also classified in terms of their frequency within the population, with common variants defined as those in which the minor allele is present at a frequency of greater than 5% in the population, while for rare variants it is present at a frequency of less than 5%. The fundamental source of genetic variation is mutation, and the majority of common genetic variants arose once in human history and are shared by many individuals today through descent from common ancient ancestors. A polymorphism is, by convention, defined as a genetic variant that is present in at least 1% of the population and thereby excludes rare variants that may have arisen in relatively recent human history. Much of the study of genetic variation to date has focused on characterizing the 10 million estimated single nucleotide polymorphisms (SNPs), as they comprise approximately 78% of human variants, thus accounting for most genetic diversity. SNPs are located, on average, every 100 to 300 bases in the genome. Structural variants account for only an estimated 22% of all variants in the genome, but they comprise an estimated 74% of the nucleotides that differ between individuals [1]. As a result of technological advances that enable their detection, there has been a flurry of recent efforts to catalogue structural polymorphisms on a genomic scale [4–6].The study of inheritance of genetic variation depends on two key concepts: genetic linkage and linkage disequilibrium (Figure 2). Two loci are in genetic linkage if they are physically close enough to one another such that recombination occurs between them with a less than 50% probability in a single generation, resulting in their co-segregation more often than if they were independently inherited (Figure 2a,b). Recombination frequency is measured in units of centimorgans, with 1 centimorgan equal to a 1% chance that two loci will segregate independently due to recombination in a single generation. One centimorgan is, on average, equivalent to 1 million base pairs (bp) in the human genome.Figure 2Identification of genetic variation underlying human disease using linkage analysis and genome-wide association studies. (a) Rare Mendelian traits, such as a monogenic disease with autosomal dominance inheritance, can be studied using linkage analysis in a family. The disease status is followed within a pedigree (seven affected individuals depicted in red). (b) The disease loci (red bar) co-segregates with the genetic marker (blue bar), located 10 centimorgans (cM) apart. Each of the seven individuals with the disease carries the blue genetic marker, both inherited from the affected 'parent' chromosome (yellow). (c) Genetic variants underlying common diseases can be statistically identified by using SNP-based linkage disequilibrium (LD) maps. The frequency of a causative variant (red diamond) will be higher (62%) among those with the disease when compared with a control population (50%). (d) LD map of 11 variants cluster into three blocks of correlation r2 > 0.8 (red scale correlation matrix). The LD between polymorphisms needs to be empirically determined by genotyping a population and calculating the correlation.Full size imageLinkage disequilibrium is a measure of the co-occurrence in a population of a particular allele at one locus with a particular allele at a second locus at a higher frequency than would be predicted by random chance. Linkage disequilibrium is created when a new mutation occurs in a genomic interval that already contains a particular variant allele, and is eroded over the course of many generations by recombination. Various statistics have been used to measure the amount of linkage disequilibrium between two variant alleles, one of the most useful being the coefficient of correlation r2. When r2 = 1 the two variant alleles are in complete linkage disequilibrium, whereas values of r2 < 1 indicate that the ancestral complete linkage disequilibrium has been eroded. Thus, while genetic linkage results from recombination in the last two to three generations and measures co-segregation in a pedigree, linkage disequilibrium depends on the association of variant alleles within a population of unrelated individuals and reflects evolutionary history (Figure 2c,d).","The first disease traits to be ascribed to particular genes were Mendelian traits, which are controlled by a single gene and follow well defined models of inheritance, such as autosomal dominant, autosomal recessive, and X-linked (Figure 2a). Genetic variants underlying Mendelian diseases are highly penetrant by definition (that is, the variant is associated with a very high relative risk of having the disease) and, as a result of negative selection, they tend to be rare (Figure 3).Figure 3The allelic spectrum of disease is dependent on the number of genetic variants, their frequency in a population and on the size of their phenotypic effect. Family-based linkage studies have proved successful in identifying causative genetic variants in rare Mendelian disorders, which are, by definition, caused by highly penetrant variants that have a low frequency in the population. Complex diseases are caused by multiple genetic variants that confer incremental risk of disease. Genome-wide association studies have sufficient power to detect genetic variants with modest phenotypic effects, provided that they occur at a high frequency in the population. Adapted from [92].Full size imageIn the 1980s and 1990s, the creation of genetic-linkage maps was based on sequence-dependent data such as restriction-fragment length polymorphisms [7, 8] and microsatellite markers [9]. These techniques established genetic-linkage analysis as the traditional method for identifying genetic variation underlying monogenic genetic disorders. Linkage studies consisted of mapping broad genetic regions that segregate with a disease in families and then using positional cloning to narrow down the candidate region in order to isolate disease-causing genes or variants. Linkage analyses were successful in identifying genetic variants in genes responsible for many notable Mendelian diseases, including cystic fibrosis [10], for which the major disease variant has a deletion of a single amino acid, Charcot-Marie-Tooth Disease Type 1A [11], for which the underlying genetic variant is a DNA duplication, and Huntington's disease [12], which is a trinucleotide repeat disorder. By 1995, genetic linkage mapping had been used to uncover variants underlying hundreds of human Mendelian traits and diseases. Thus, almost a decade before the elucidation of the human genome sequence, it was fully appreciated that DNA variants of all classes, both common and rare as well as sequence and structural, play important roles in single-gene traits and rare Mendelian diseases.The next, and more difficult, stage was to determine genes associated with the far more common complex (multigene) diseases such as diabetes, heart disease and cancer. The conceptual framework for statistical association studies to identify common genetic variants underlying common diseases was established by Risch and Merikangas in 1996 [13], and is now referred to as the common disease/common variant (CD/CV) hypothesis. This hypothesis states that common diseases are caused by multiple genetic variants that are present at a high frequency in the population and confer cumulative incremental effects on disease risk (Figure 3) [14, 15]. It is thought that due to the low penetrance and modest risk associated with these common variant alleles, they do not undergo the same strong negative selection as highly penetrant rare variants underlying Mendelian diseases. In addition, environment and behavior are believed to contribute over 70% of the susceptibility to diseases such as cancer, coronary heart disease and type 2 diabetes [16]. On the basis of these assumptions in the CD/CV model, it was posited that to identify variant that occur at a high frequency in the population yet confer a small risk for disease, it would be feasible to use SNP-based linkage disequilibrium maps to survey the common genetic variation present in the entire genomes of a large number of individuals.Several key technological advances laid the foundation for the eventual successful implementation of genome-wide association studies in identifying common genetic variants underlying complex traits. The first was the completion of the 3 billion bp human genome sequence in 2001, which served as a reference sequence to which genotype or sequence information from individuals could be compared [17, 18]. Then, large-scale efforts led to the discovery of a substantial fraction of the 10 million estimated SNPs in the human population. By genotyping millions of these SNPs in hundreds of individuals, the International HapMap Project created SNP linkage disequilibrium maps, reducing the vast majority of common genetic variation in the 3 billion bp human genome to around 500,000 tag SNPs that are proxies for other SNPs in high linkage disequilibrium [19]. This resource has driven a wave of critical technological advances in the design of genome-wide SNP arrays that allow the rapid and cost-effective genotyping of hundreds of thousands to millions of tag SNPs in each individual, thus allowing the examination of common genetic variation across the genome.Genome-wide association studies using SNP-based arrays compare the frequency of SNP alleles in the genomes of a group of individuals with a complex trait (the cases) to a control group (Figure 2c). This approach allows the identification of common genetic variants that are either causative or in linkage disequilibrium with a causative allele. In reviewing the design of successful genome-wide association studies, three key features become clear. First, because of the moderate risk conferred by many common genetic variants, it is imperative to design an adequately powered study with large sample sizes that are carefully controlled to minimize bias [20–22]. Second, SNP selection and detection is critical, and there is an ongoing effort to catalog more SNPs across the genome and to create methods to assay SNP genotypes more densely. Finally, even statistically convincing associations require validation by replication in an independent cohort.","During 2007, the first wave of genome-wide association studies using tag SNPs resulted in the identification of common genetic variants associated with a broad range of common diseases and traits, including cancer, metabolic diseases, immune-mediated diseases and neurodegenerative diseases (Table 1). The findings of these genome-wide scans can best be reviewed by discussing the results of studies investigating specific complex diseases and traits. Gout and its associated serum uric acid concentration has been studied in two genome-wide association studies [23, 24], resulting in the identification of variants in the gene SLC2A9 (solute carrier family 2 member 9). SLC2A9 variants were associated with high concentration of uric acid in the serum (between 1.7% and 5.3% increase) and the expression level of the isoform 2 of SLC2A9 was correlated with serum uric acid concentration [24]. This isoform encodes the protein Glut9ΔN, a putative fructose transporter expressed in kidney. As fructose is upstream in the pathway generating uric acid, an impaired expression of this protein possibly leads to the increased level of serum uric acid observed in gout [23, 24].Table 1 Genetic loci associated with disease and phenotypic variationFull size tableMultiple genome-wide association studies investigating coronary artery disease have independently identified a strong association with SNPs in a chromosomal region at 9p21. Individuals homozygous for the 9p21 risk allele have a 1.9 higher relative risk of suffering from coronary artery disease than individuals homozygous for the non-risk alleles [22, 25–28]. Interestingly, this region does not harbor any known genes, and the underlying biological reason for the association is unknown. Beyond diseases, genome-wide scans have identified variants associated with human height: HMG2A (a transcription factor) and GDF5-UQCC (a locus associated with osteoarthritis) [29, 30]. In addition, variants in FTO (fat mass and obesity associated gene) have been associated with obesity: adults homozygous for the risk allele have an increased relative risk of 1.67 for being obese compared with the non-risk allele carriers [31].In spite of the exciting successes of recent SNP-based genome scans, the results of studies investigating specific complex diseases indicate that the approach frequently identifies common variants that account for only a small fraction (less than 10%) of the heritable component of the disease [32]. Most of the associated SNPs typically result in an increased relative risk of around 1.2 for heterozygotes and for many diseases only a few SNPs have been identified. Thus, we are left asking where is the remaining genetic variance underlying these heritable diseases? It is likely that some of this missing variation is accounted for by common variants with very small effects, which the current studies, despite the rather large cohorts used, are not powerful enough to capture. The additive or even multiplicative integrated effect of common SNPs may be important, as recently shown with five SNPs that increase susceptibility to prostate cancer [33]. Such gene-gene interactions are typically not accounted for in the analysis of genome scans. It is well established that SNP-based genome scans have limited power to capture the association of rare variants, which are likely to be important contributors to complex diseases. Structural variants have been demonstrated to underlie phenotypic diversity of complex traits [34, 35] but have not generally been captured with current SNP-centric platforms for ultra-high throughput genotyping. Recent studies have shown that this class of variants is enriched in segmentally duplicated regions of the genome, in which there is a paucity of tag SNPs because of technical difficulties [36]. Thus, the missing variation in SNP-based genome scans indicates that systematically examining these other types of variants for their contribution to complex diseases is important.","Although the discoveries of SNP-based genome-wide association studies are exciting, it is important to note that they are limited to the statistical association of DNA variants with common diseases and that the biological mechanisms underlying most of these findings are not yet known. For example, multiple studies have shown that three SNPs on chromosome 16p13 in the vicinity of KIAA0350 are unequivocally associated with type 1 diabetes, but it is unclear how the risk and non-risk alleles differ; is it in expression, alternative splicing patterns, or the function of the protein encoded by KIAA0350? [37] This uncertainty in the underlying biological cause of an association is especially pronounced when the variant lies in a chromosomal interval that does not contain a gene, such as the association of the 9p21 interval with coronary artery disease. Therefore, the findings of most association studies currently can only be used for crude predictions of the likelihood that an individual will develop a certain disease.To translate the findings of SNP-based genome scans into clinical practice to improve human health, it is necessary to establish new, highly innovative approaches for assaying intervals containing associated variants for functional differences between the risk and non-risk alleles. This will require access to diverse and large patient populations to obtain biological samples. Each genomic interval has a different landscape of functional sequences, and this, together with the fact that each disease affects different biological processes, makes it impossible to develop a 'one-size-fits-all' strategy to annotate associated sequences for functional differences between risk and non-risk alleles. Thus, it is also essential to make use of diverse experimental methods and technologies in all the various biological 'omics': genomics, proteomics, epigenomics, metabolomics, structural genomics and glycomics.Several public and private initiatives are developing 'next generation' sequencing technologies based on pyrosequencing (Roche-454) [38], sequencing by synthesis (Illumina-Solexa) [39] or sequencing by ligation (ABI-SOLiD). These technologies, capable of the cost-effective generation of massive amounts of DNA sequence, are already being used to sequence targeted regions, and in the near future will be capable of sequencing whole genomes of individuals to simultaneously examine SNPs and other genetic variants for associations with specific diseases. The statistical analysis methods for assessing the relationship between rare genetic variants identified in sequence data and complex traits are beginning to be developed. Results of sequence-based studies conducted so far suggest that associated intervals will be identified on the basis that the frequency of rare genetic variants with functional consequences will be greater in individuals with the complex disease versus controls. Thus, next-generation sequencing technologies, by detecting a myriad more SNPs and other types of variation associated with complex disease, will increase the difficulty and at the same time, the importance of functional annotation of genetic variants. At this point, it appears that we are just beginning to appreciate the extent of human genomic variation. Projects like the '1000 Genomes' and large-scale efforts to perform deep-coverage sequencing in both healthy patients and those with complex traits will help propel this exciting field further."
